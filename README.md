# Shaidow

**Terminal companion that "shadows" your CLI work with real-time AI assistance.**

Shaidow splits your terminal in half, allowing you to work like you normally would on one pane and receive live AI feedback in the other. The LLM assistant watches your commands and their outputs and provides contextual insights, suggestions, and troubleshooting help. Optimized for SREs working on OpenShift clusters.

## Features

- **Real-time Command Analysis**: LLM observes your shell commands and provides immediate feedback
- **Intelligent Suggestions**: Get relevant follow-up commands and investigation paths
- **Inline DMs**: Use `# comments` to directly communicate with the LLM
- **SOP Integration**: Load Standard Operating Procedures to guide AI responses
- **LLM Agnostic**: Works with any model supported by the [`llm` Python package](https://llm.datasette.io/en/stable/) (e.g., Gemini, ChatGPT, and local models)

## Setup

Shaidow relies on the following dependencies:

- Python 3.10+
- `tmux`, `sed`, and `script` (probably pre-installed, otherwise avilable via your friendly package manager)
  - Note that `script` might be hidden within a package named something like `util-linux`
- `llm` and `rich` Python packages (`pip3 install -r requirements.txt`)
- [`script2json` Golang tool](https://github.com/abyrne55/script2json) for converting `script`'s output into JSON

Use the following steps to install and configure these dependencies (excluding Python; you're on your own for that).

1. Clone this repo and `cd` into it
2. (optional) Create and activate a Python virtual environment
   ```sh
   python3 -m venv .venv
   source .venv/bin/activate
   ```
2. Install required dependencies
   ```sh
   pip3 install -r requirements.txt
   # replace dnf with apt, brew, etc. as needed
   sudo dnf install tmux script sed
   go get github.com/abyrne55/script2json
   ```

4. Configure `llm` for use with a remote model provider (e.g., Google Gemini, OpenAI ChatGPT) or local model manager (e.g., gpt4all). See [the `llm` docs](https://llm.datasette.io/en/stable/setup.html#installing-plugins) for more details
   ```bash
   # ChatGPT (no llm plugin needed)
   llm keys set openai
   llm -m o3 "Hello World"

   # Gemini
   llm install llm-gemini
   llm keys set gemini
   llm -m gemini-2.5-flash "Hello World"

   # Claude
   llm install llm-anthropic
   llm keys set anthropic
   llm -m claude-4-sonnet "Hello World"
   
   # Or download and run local models
   llm install llm-gpt4all
   llm -m orca-mini-3b-gguf2-q4_0 "Hello World"
   ```

## Usage

Run `./start.sh -m $MODEL_NAME`, specifying any of the LLM names/aliases shown by `llm list models`. Running start.sh without any arguments will default to using Google's Gemini 2.5 Pro. Note that start.sh will emit some warnings if `llm` hasn't been pre-configured with any remote model API keys. If you're only using local models, you can silence these warnings by setting `SKIP_KEY_CHECK=1` before running start.sh. 

This script will spawn a two-pane tmux session that works something like the following:
```
┌─────────────────┬─────────────────┐
│------Shell------│--AI Assistant---│
│                 │                 │
│ $ oc get pods   │                 │
│                 │ xyz pod looks   │
│                 │  unhealthy, no? │
│ $ # How should  |                 │
│    I debug xyz? │ Let's start with│
│                 │  oc logs xyz    │
│ $ oc logs xyz   │                 │
└─────────────────┴─────────────────┘
```
In other words, type into the left pane as you would a normal shell (see _Technical Details > Shell_ below). Each time you run a command, the command and its output will be sent to the LLM. Any response/feedback generated by the LLM will be displayed in the right pane. You can also type `# shell comments` into the right pane to chat directly with the LLM.

To return to your normal shell, run `exit` or press Ctrl-D.

### `#` Commands
#### Direct Messaging the Assistant (`# DMs`)
You can communicate directly with the LLM by typing `#` followed by your message. These comments are forwarded straight to the assistant without being executed as shell commands. Use this to ask questions, provide context, or respond to the assistant's queries. Remember that some LLMs (e.g., Gemini) have the ability to search the web or execute Python code on your behalf.

#### Paste Suggested Commands (`##`)
Type `##` to automatically paste the last command suggested by the LLM into your shell prompt. Use `###` for the second-to-last suggested command, `####` for the third-to-last, and so on. This feature requires the `--tmux-shell-pane` flag to be  set to the ID of your tmux shell pane (`start.sh` does this for you).

#### Ignored Commands (`#i`)
Append `#i` to any command to prevent the assistant from seeing or analyzing it. This is useful for running unrelated commands or interactive programs (like `watch` or `top`) that would clutter the assistant's context. The command still executes normally in your shell.

### Injecting SOPs
Use the `-s` flag to provide local copies of Markdown-formatted standard operating procedure (SOP) documents to the LLM. You can provide as many SOPs as you'd like; just keep in mind the context window size of whichever model you're using.
```bash
./start.sh -s /path/to/incident-response.md -s /path/to/NodeNotReady.md
```
Once inside Shaidow, you can query the LLM about any SOPs you've provided.
```
$ oc get nodes
NAME           STATUS     ROLES    AGE   VERSION
node1          Ready      worker   10d   v1.28.2
node2          NotReady   worker   10d   v1.28.2
node3          Ready      master   10d   v1.28.2

[LLM Response] Looks like node2 is having trouble. The SOP suggests running `oc debug no/node2` next

$ # I just checked the AWS console, and it looks like node2 has been terminated. Does the SOP say anything about that?

[LLM Response] Ah, that would explain why it's NotReady! Yes, the SOP says to contact the cluster administrator and ask them to...
```

### Other Options
Any flags passed to start.sh are passed along to shaidow.py (a.k.a. what runs in the "right pane"). Run `python3 shaidow.py -h` for more information about the flags it accepts
```
$ python3 shaidow.py -h
usage: shaidow.py [-h] [--fifo FIFO] [--sop SOP] [--verbose] [--model MODEL] [--sysprompt-only-once] [--tmux-shell-pane TMUX_SHELL_PANE]

SRE assistant that reads commands from a FIFO

options:
  -h, --help            show this help message and exit
  --fifo, -f FIFO       Path to FIFO file (will be created if it does not exist)
  --sop, -s SOP         Path to a local copy of an SOP to add to the LLM's context window
  --verbose, -v         Verbose output (including LLM token usage)
  --model, -m MODEL     LLM to use (default: gemini-2.5-pro)
  --sysprompt-only-once, -p
                        only provide the system prompt during LLM initialization — helpful for models with small context windows (default: provide sysprompt with each request)
  --tmux-shell-pane, -t TMUX_SHELL_PANE
                        ID of the tmux pane containing the shell being recorded
```

## Technical Details

### Overview
1. **Session Setup**: `start.sh` creates a secure tmux session with a "shell pane" running `script` on the left and an "assistant pane" running `shaidow.py` on the right
2. **FIFO Communication**: A few named pipes (FIFOs) created within a temporary directory connects the shell and assistant panes, with [script2json](https://github.com/abyrne55/script2json) as the middle man
3. **Command Logging**: The shell pane gets configured with Bash debugging "traps" that enable script2json to capture your commands/outputs and write them into Shaidow's input FIFO in JSONL format
4. **AI Processing**: `shaidow.py` analyzes each command/output using your chosen LLM
5. **Real-time Feedback**: The AI provides contextual insights and suggestions

### Shell
It turns out that live-recording shell commands and their outputs in a structured format is not a use case well-served by a single widely-available tool. Shaidow relies on `script`, `script2json`, FIFOs, and Bash features (specifically `$PROMPT_COMMAND`, the `DEBUG` trap, and the `fc` built-in) in order to satisfy this use case while still providing users with a fully-functional Bash shell. This approach carries some limitations.
* **Race Conditions**: `script2json` relies on the shell to send signals (i.e., SIGUSR1 and SIGUSR2) at precise moments in order to differentiate between user input and command output. While not frequently encountered during testing, these signals can arrive late or out-of-order when your system is under heavy load, causing commands/outputs to be missed, corrupted, or desynchronized. Restarting Shaidow will fix most cases like this. Consider also `renice`ing the `script` and `script2json` processes
* **Interactive Commands**: Full-screen interactive programs like `watch`, `top`, `vim`, or `htop` don't work well with Shaidow's recording mechanism. Their dynamic output isn't captured in a structured format, so the LLM won't see meaningful updates. Append `#i` to these commands to prevent the assistant from trying to analyze them.
* **Subshells (e.g., `ssh`, `oc debug`)**: Commands run inside subshells (like `ssh host` or `oc debug node/xyz`) may not be captured and sent to the assistant until the subshell exits (if at all). The new shell lacks the Bash instrumentation (DEBUG trap, PROMPT_COMMAND, signal handlers) that enables command recording. Consider running individual commands with `-c` flags (e.g., `ssh host "command"`) instead of entering interactive subshells.

### System Prompt 
The AI is optimized for SRE workflows but can be customized by editing the `system_prompt` in `shaidow.py`. Here's what Claude 4 Sonnet says when asked to summarize the "stock" system prompt:
> The system prompt defines an AI assistant designed to help site reliability engineers (SREs) investigate OpenShift 4 cluster problems by analyzing shell command outputs and highlighting important information they might miss. The assistant is programmed to suggest follow-up commands, respond to direct messages via shell comments, and ask clarifying questions while maintaining a concise communication style of less than 19 words on average. It can incorporate standard operating procedures (SOPs) into its responses and offers to help update them, while always deferring to the SRE's judgment and avoiding responses when it cannot meaningfully contribute to the investigation.

### SOPs
SOPs provided by the user are given to `llm` as ["fragments"](https://llm.datasette.io/en/stable/fragments.html) attached to the "Hello" query that's used to initialize the model. Every LLM plugin handles fragments a little differently, but in practice, it doesn't seem all that much different from just running `cat /path/to/sop.md` in Shaidow's left pane.
