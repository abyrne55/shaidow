# Shaidow

**Terminal companion that "shadows" your CLI work with real-time AI assistance.**

Shaidow splits your terminal in half, allowing you to work like you normally would on one pane and receive live AI feedback in the other. The LLM assistant watches your commands and their outputs and provides contextual insights, suggestions, and troubleshooting help. Optimized for SREs working on OpenShift clusters.

## Features

- **Real-time Command Analysis**: LLM observes your shell commands and provides immediate feedback
- **Intelligent Suggestions**: Get relevant follow-up commands and investigation paths
- **Inline DMs**: Use `# comments` to directly communicate with the LLM
- **SOP Integration**: Load Standard Operating Procedures to guide AI responses
- **LLM Agnostic**: Works with any model supported by the [`llm` Python package](https://llm.datasette.io/en/stable/) (e.g., Gemini, ChatGPT, and local models)

## Setup

Shaidow relies on the following dependencies:

- Python 3.10+
- `tmux`, `jq`, and `script` (probably pre-installed, otherwise avilable via your friendly package manager)
  - Note that `script` might be hidden within a package named something like `util-linux`
- `llm` and `rich` Python packages (`pip3 install -r requirements.txt`)

Use the following steps to install and configure these dependencies (excluding Python; you're on your own for that).

1. Clone this repo and `cd` into it
2. (optional) Create and activate a Python virtual environment
   ```sh
   python3 -m venv .venv
   source .venv/bin/activate
   ```
2. Install required dependencies
   ```sh
   pip3 install -r requirements.txt
   # replace dnf with apt, brew, etc. as needed
   sudo dnf install tmux jq script 
   ```

4. Configure `llm` for use with a remote model provider (e.g., Google Gemini, OpenAI ChatGPT) or local model manager (e.g., gpt4all). See [the `llm` docs](https://llm.datasette.io/en/stable/setup.html#installing-plugins) for more details
   ```bash
   # ChatGPT (no llm plugin needed)
   llm keys set openai
   llm -m o3 "Hello World"

   # Gemini
   llm install llm-gemini
   llm keys set gemini
   llm -m gemini-2.5-flash "Hello World"

   # Claude
   llm install llm-anthropic
   llm keys set anthropic
   llm -m claude-4-sonnet "Hello World"
   
   # Or download and run local models
   llm install llm-gpt4all
   llm -m orca-mini-3b-gguf2-q4_0 "Hello World"
   ```

## Usage

Run `./start.sh -m $MODEL_NAME`, specifying any of the LLM names/aliases shown by `llm list models`. Running start.sh without any arguments will default to using Google's Gemini 2.5 Pro. Note that start.sh will emit some warnings if `llm` hasn't been pre-configured with any remote model API keys. If you're only using local models, you can silence these warnings by setting `SKIP_KEY_CHECK=1` before running start.sh. 

This script will spawn a two-pane tmux session that works something like the following:
```
┌─────────────────┬─────────────────┐
│------Shell------│--AI Assistant---│
│                 │                 │
│ $ oc get pods   │                 │
│                 │ xyz pod looks   │
│                 │  unhealthy, no? │
│ $ # How should  |                 │
│    I debug xyz? │ Let's start with│
│                 │  oc logs xyz    │
│ $ oc logs xyz   │                 │
└─────────────────┴─────────────────┘
```
In other words, type into the left pane as you would a normal shell (with caveats... see _Shell Limitations_ below). Each time you run a command, the command and its output will be sent to the LLM. Any response/feedback generated by the LLM will be displayed in the right pane. You can also type `# shell comments` into the right pane to chat directly with the LLM.

To return to your normal shell, run `exit` or press Ctrl-C.

### Shell Limitations
Unfortunately, the left pane is not a normal shell. It's actually `shell.sh`, which just simulates a shell in order to capture the commands going in and the output coming back. This results in a _very_ barebones shell experience, i.e., no environmental variables, no autocomplete, no history, not even proper arrow-key-based line editing. It will also lock up if you run any command that doesn't exit on its own accord (e.g. `oc logs -f`). We're working on something better; if you're willing to experiment a bit, see https://github.com/abyrne55/script2json

### Injecting SOPs
Use the `-s` flag to provide local copies of Markdown-formatted standard operating procedure (SOP) documents to the LLM. You can provide as many SOPs as you'd like; just keep in mind the context window size of whichever model you're using.
```bash
./start.sh -s /path/to/incident-response.md -s /path/to/NodeNotReady.md
```
Once inside Shaidow, you can query the LLM about any SOPs you've provided.
```
$ oc get nodes
NAME           STATUS     ROLES    AGE   VERSION
node1          Ready      worker   10d   v1.28.2
node2          NotReady   worker   10d   v1.28.2
node3          Ready      master   10d   v1.28.2

[LLM Response] Looks like node2 is having trouble. The SOP suggests running `oc debug no/node2` next

$ # I just checked the AWS console, and it looks like node2 has been terminated. Does the SOP say anything about that?

[LLM Response] Ah, that would explain why it's NotReady! Yes, the SOP says to contact the cluster administrator and ask them to...
```

### Other Options
Any flags passed to start.sh are passed along to shaidow.py (a.k.a. what runs in the "right pane"). Run `python3 shaidow.py -h` for more information about the flags it accepts
```
$ python3 shaidow.py -h
usage: shaidow.py [-h] [--fifo FIFO] [--sop SOP] [--verbose] [--model MODEL]

SRE assistant that reads commands from a FIFO

options:
  -h, --help         show this help message and exit
  --fifo, -f FIFO    Path to FIFO file (will be created if it does not exist)
  --sop, -s SOP      Path to a local copy of an SOP to add to the LLM's context window
  --verbose, -v      Verbose output (including LLM token usage)
  --model, -m MODEL  LLM to use
```

## Technical Details

### Overview
1. **Session Setup**: `start.sh` creates a secure tmux session with two panes
2. **FIFO Communication**: A named pipe (FIFO) created within a temporary directory connects the shell and AI panes
3. **Command Logging**: `shell.sh` captures your commands/outputs and writes them into the FIFO in JSONL format
4. **AI Processing**: `shaidow.py` analyzes each command/output using your chosen LLM
5. **Real-time Feedback**: The AI provides contextual insights and suggestions

### System Prompt 
The AI is optimized for SRE workflows but can be customized by editing the `system_prompt` in `shaidow.py`. Here's what Claude 4 Sonnet says when asked to summarize the "stock" system prompt:
> The system prompt defines an AI assistant designed to help site reliability engineers (SREs) investigate OpenShift 4 cluster problems by analyzing shell command outputs and highlighting important information they might miss. The assistant is programmed to suggest follow-up commands, respond to direct messages via shell comments, and ask clarifying questions while maintaining a concise communication style of less than 19 words on average. It can incorporate standard operating procedures (SOPs) into its responses and offers to help update them, while always deferring to the SRE's judgment and avoiding responses when it cannot meaningfully contribute to the investigation.

### SOPs
SOPs provided by the user are given to `llm` as ["fragments"](https://llm.datasette.io/en/stable/fragments.html) attached to the "Hello" query that's used to initialize the model. Every LLM plugin handles fragments a little differently, but in practice, it doesn't seem all that much different from just running `cat /path/to/sop.md` in Shaidow's left pane.
